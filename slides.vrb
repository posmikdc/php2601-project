\frametitle{EDA and the Linear Model}
\phantomsection\label{eda-and-the-linear-model}
\begin{block}{Introduction}
\phantomsection\label{introduction}
We will be analyzing educational data to understand the predictors of
student performance. Specifically, we seek to \textbf{understand whether
five predictors -- as a subset of an exhaustive list of potential
predictors -- are significant predictors of student performance}.

Testing the significant of a subset of predictors is becoming
increasingly important in modern statistical questions, especially with
more information becoming available.

We will be using a publicly available dataset from Kaggle that contains
information about students and their exam scores.
\end{block}

\begin{block}{Hypothesis to be Tested}
\phantomsection\label{hypothesis-to-be-tested}
We are interested in:

\begin{itemize}
\tightlist
\item
  Hours Studied
\item
  Attendance
\item
  Sleep Hours
\item
  Previous Scores
\item
  Tutoring Sessions
\end{itemize}

We can formalize this question as follows:

\begin{itemize}
\tightlist
\item
  \(H_0: \begin{bmatrix} 1_{[0, \cdots, p+1]}, & 0_{[p+2, \cdots, P]} \end{bmatrix} \cdot \begin{bmatrix} \beta_0 & \cdots & \beta_{P} \end{bmatrix}^T = \beta_0 + \cdots + \beta_{p+1} = 0\)
\item
  \(H_A: \{\beta_1 \neq 0\} \cap \cdots \cap \{\beta_5 \neq 0\}\)
\end{itemize}

Observe the 0-indexed variables from \(p+2\) to \(P\).
\end{block}

\begin{block}{Exploratory Data Analysis (EDA)}
\phantomsection\label{exploratory-data-analysis-eda}
\begin{figure}[H]

{\centering \includegraphics{correlation_matrix.png}

}

\caption{Correlation Matrix}

\end{figure}%
\end{block}

\begin{block}{Variable Transformations}
\phantomsection\label{variable-transformations}
We will transform the variables to ensure that the assumptions of the
linear model are met.

\begin{figure}[H]

{\centering \includegraphics{histograms.png}

}

\caption{Variable Transformation}

\end{figure}%

We use log and square root transformations to ensure that the residuals
are normally distributed.
\end{block}

\begin{block}{The Linear Model}
\phantomsection\label{the-linear-model}
Let us begin by discussing the assumptions of linear regression model.
In a Gauss-Markov setting, we assume that our linear model is of the
form:

\[
Y = \begin{bmatrix} Y_1 \\ Y_2 \\ \vdots \\ Y_n \end{bmatrix} =
\begin{bmatrix}
1 & X_{12} & X_{13} & \cdots & X_{1(p+1)} \\
1 & X_{22} & X_{23} & \cdots & X_{2(p+1)} \\
\vdots & \vdots & \vdots & \ddots & \vdots \\
1 & X_{n2} & X_{n3} & \cdots & X_{n(p+1)}
\end{bmatrix}
\begin{bmatrix} \beta_0 \\ \beta_1 \\ \beta_2 \\ \vdots \\ \beta_p \end{bmatrix} +
\begin{bmatrix} \epsilon_1 \\ \epsilon_2 \\ \vdots \\ \epsilon_n \end{bmatrix}
\]

where \(\mathbb{E}[\epsilon] = 0\) and
\(\text{Var}[\epsilon] = \sigma^2I\) denote the zero-mean and constant
variance assumptions. In our case, we begin with \(p = 5\), i.e.~our
design matrix has \(p+1\) columns, accounting for the intercept term.
\end{block}

\begin{block}{Solving for \(\hat{\beta}\)}
\phantomsection\label{solving-for-hatbeta}
We can solve for \(\hat{\beta}\) via the normal equations:

\[
\begin{aligned}
\hat{\beta} = &(X^TX)^{g}X^TY \\
= &\left(
\begin{bmatrix} 1 & 1 & \cdots & 1 \\ X_{12} & X_{22} & \cdots & X_{n2} \\ \vdots & \vdots & \ddots & \vdots \\ X_{1(p+1)} & X_{2(p+1)} & \cdots & X_{n(p+1)} \end{bmatrix}
\begin{bmatrix} 1 & X_{12} & \cdots & X_{1(p+1)} \\ 1 & X_{22} & \cdots & X_{2(p+1)} \\ \vdots & \vdots & \ddots & \vdots \\ 1 & X_{n2} & \cdots & X_{n(p+1)} \end{bmatrix}
\right)^{g}
\cdot \\
&\begin{bmatrix} 1 & 1 & \cdots & 1 \\ X_{12} & X_{22} & \cdots & X_{n2} \\ \vdots & \vdots & \ddots & \vdots \\ X_{1(p+1)} & X_{2(p+1)} & \cdots & X_{n(p+1)} \end{bmatrix}
\begin{bmatrix} Y_1 \\ Y_2 \\ \vdots \\ Y_n \end{bmatrix}
\end{aligned}
\]

In our case, all predictors but Sleep Hours are significant predictors
of exam scores, even at a 1\% level of significance.
\end{block}

\begin{block}{Estimability of the Hypothesis}
\phantomsection\label{estimability-of-the-hypothesis}
Question: \textbf{Can we estimate an object \(K^T \beta\) with our data
\(X\)?}

Formally, we say that if \(\exists ~ A ~ \text{s.t. } X^TA = K^T\),
i.e.~\(K^T\) can be expressed as a linear combination of \(X\) and some
matrix \(A\), then \(K^T \beta\) is estimable.

In our case, this is straightforward to verify. Can we think of an
example when this is not true? (Hint: Dimension ``mismatch'')
\end{block}

\begin{block}{Distribution of \(K^T \beta\)}
\phantomsection\label{distribution-of-kt-beta}
Since \(K^T \beta\) estimable, its best linear unbiased estimator (BLUE)
is given by:

\[
\begin{aligned}
\mathbf{K_i}^T \hat{\beta} &\sim \textit{N}(\mathbf{K_i}^T (X^T X)^g X^T X \beta, \sigma^2 \mathbf{K_i}^T(X^TX)^{g}\mathbf{K_i}) \quad \text{and} \\
\mathbf{K}^T \hat{\beta} &\sim \textit{N}(\mathbf{K}^T (X^T X)^g X^T X \beta, \sigma^2 \mathbf{K}^T(X^TX)^{g}\mathbf{K})
\end{aligned}
\]

This object \(K^T \beta\) may seem a bit arbitrary, even useless, at
first. However, it is in fact the building block for the test statistic
we will construct now!
\end{block}

\begin{block}{Quadratic Form in our Joint Testing Procedure}
\phantomsection\label{quadratic-form-in-our-joint-testing-procedure}
Suppose \(H := K (X^T X)^g K^T\), then

\[
\begin{aligned}
(K \beta)^T (\sigma^2 H)^{-1} (K \hat{\beta}) &\sim \chi^2_{\text{df} = \text{rank}(H)}(\lambda) \\
\end{aligned}
\]

where the non-centrality parameter
\(\lambda = \frac{1}{2} (K \beta)^T (\sigma^2 H)^{-1}(K \beta)\) is the
well-known distributional result of a normal quadratic form.

Finally, our F Statistic:

\[
\begin{aligned}
F := \frac{\left((K \beta)^T (\sigma^2 H)^{-1}(K \beta)\right) / \text{rank}(H)}{\text{RSS}/(n-p)}
\sim \frac{\chi^2 (\lambda)}{\chi^2} \sim F_{\text{rank}(H), n-p}(\lambda)
\end{aligned}
\]

We have successfully constructed a statistical test that allows us to
test our hypothesis with a simple F-test. In \texttt{R}, we can use the
\texttt{anova()} function to perform this test.
\end{block}

\begin{block}{Results}
\phantomsection\label{results}
\begin{figure}[H]

{\centering \includegraphics{anova-results.png}

}

\caption{F-Test Results}

\end{figure}%

The result shows that under the null hypothesis, the probability of
getting a more extreme result than our calculate F-test statistics
\(\text{Pr}(>F)\) is \(2.2eâˆ’ 16\).

This evidence would lead us to reject the null hypothesis and conclude
that our subset of predictors is indeed a significant predictor of exam
scores
\end{block}
