---
title: "Predicting Student Performance"
subtitle: "Linear Models (PHP2601), Prof. Ani Eloyan"
author: "Daniel Posmik, Jizhou Tian, Aristofanis Rontogiannis"
toc: true
date: today 
format: beamer
---

```{r setup, include=FALSE}
# Set up knit environment
knitr::opts_chunk$set(echo = F)
knitr::opts_chunk$set(error = F)
knitr::opts_chunk$set(warning = F)
knitr::opts_chunk$set(message = F)

# Load necessary packages
library(tidyverse)
library(kableExtra)
library(knitr)
library(broom)
library(ggplot2)
library(naniar)
library(gtsummary)
library(GGally)
library(MASS)

```

# EDA and the Linear Model 

## Introduction

We will be analyzing educational data to understand the predictors of student performance. Specifically, we seek to **understand whether five predictors -- as a subset of an exhaustive list of potential predictors -- are significant predictors of student performance**.

Testing the significant of a subset of predictors is becoming increasingly important in modern statistical questions, especially with more information becoming available. 

We will be using a publicly available dataset from Kaggle that contains information about students and their exam scores.

## Hypothesis to be Tested 

We are interested in: 

- Hours Studied
- Attendance 
- Sleep Hours 
- Previous Scores
- Tutoring Sessions

We can formalize this question as follows:

- $H_0: \begin{bmatrix} 1_{[0, \cdots, p+1]}, & 0_{[p+2, \cdots, P]} \end{bmatrix} \cdot \begin{bmatrix} \beta_0 & \cdots & \beta_{P} \end{bmatrix}^T = \beta_0 + \cdots + \beta_{p+1} = 0$ 
- $H_A: \{\beta_1 \neq 0\} \cap \cdots \cap \{\beta_5 \neq 0\}$

Observe the 0-indexed variables from $p+2$ to $P$.

## Exploratory Data Analysis (EDA)

![Correlation Matrix](correlation_matrix.png)

## Variable Transformations

We will transform the variables to ensure that the assumptions of the linear model are met.

![Variable Transformation](histograms.png)

We use log and square root transformations to ensure that the residuals are normally distributed.

## The Linear Model 

Let us begin by discussing the assumptions of linear regression model. In a Gauss-Markov setting, we assume that our linear model is of the form:

$$
Y = \begin{bmatrix} Y_1 \\ Y_2 \\ \vdots \\ Y_n \end{bmatrix} = 
\begin{bmatrix}
1 & X_{12} & X_{13} & \cdots & X_{1(p+1)} \\
1 & X_{22} & X_{23} & \cdots & X_{2(p+1)} \\
\vdots & \vdots & \vdots & \ddots & \vdots \\
1 & X_{n2} & X_{n3} & \cdots & X_{n(p+1)}
\end{bmatrix} 
\begin{bmatrix} \beta_0 \\ \beta_1 \\ \beta_2 \\ \vdots \\ \beta_p \end{bmatrix} + 
\begin{bmatrix} \epsilon_1 \\ \epsilon_2 \\ \vdots \\ \epsilon_n \end{bmatrix}
$$

where $\mathbb{E}[\epsilon] = 0$ and $\text{Var}[\epsilon] = \sigma^2I$ denote the zero-mean and constant variance assumptions. In our case, we begin with $p = 5$, i.e. our design matrix has $p+1$ columns, accounting for the intercept term.

## Solving for $\hat{\beta}$

We can solve for $\hat{\beta}$ via the normal equations:

$$
\begin{aligned}
\hat{\beta} = &(X^TX)^{g}X^TY \\
= &\left( 
\begin{bmatrix} 1 & 1 & \cdots & 1 \\ X_{12} & X_{22} & \cdots & X_{n2} \\ \vdots & \vdots & \ddots & \vdots \\ X_{1(p+1)} & X_{2(p+1)} & \cdots & X_{n(p+1)} \end{bmatrix} 
\begin{bmatrix} 1 & X_{12} & \cdots & X_{1(p+1)} \\ 1 & X_{22} & \cdots & X_{2(p+1)} \\ \vdots & \vdots & \ddots & \vdots \\ 1 & X_{n2} & \cdots & X_{n(p+1)} \end{bmatrix} 
\right)^{g} 
\cdot \\
&\begin{bmatrix} 1 & 1 & \cdots & 1 \\ X_{12} & X_{22} & \cdots & X_{n2} \\ \vdots & \vdots & \ddots & \vdots \\ X_{1(p+1)} & X_{2(p+1)} & \cdots & X_{n(p+1)} \end{bmatrix} 
\begin{bmatrix} Y_1 \\ Y_2 \\ \vdots \\ Y_n \end{bmatrix}
\end{aligned}
$$

In our case, all predictors but Sleep Hours are significant predictors of exam scores, even at a 1% level of significance. 

## Estimability of the Hypothesis

Question: **Can we estimate an object $K^T \beta$ with our data $X$?**

Formally, we say that if $\exists ~ A ~ \text{s.t. } X^TA = K^T$, i.e. $K^T$ can be expressed as a linear combination of $X$ and some matrix $A$, then $K^T \beta$ is estimable.

In our case, this is straightforward to verify. Can we think of an example when this is not true? (Hint: Dimension "mismatch")

## Distribution of $K^T \beta$

Since $K^T \beta$ estimable, its best linear unbiased estimator (BLUE) is given by: 

$$
\begin{aligned}
\mathbf{K_i}^T \hat{\beta} &\sim \textit{N}(\mathbf{K_i}^T (X^T X)^g X^T X \beta, \sigma^2 \mathbf{K_i}^T(X^TX)^{g}\mathbf{K_i}) \quad \text{and} \\ 
\mathbf{K}^T \hat{\beta} &\sim \textit{N}(\mathbf{K}^T (X^T X)^g X^T X \beta, \sigma^2 \mathbf{K}^T(X^TX)^{g}\mathbf{K}) 
\end{aligned}
$$

This object $K^T \beta$ may seem a bit arbitrary, even useless, at first. However, it is in fact the building block for the test statistic we will construct now! 

## Quadratic Form in our Joint Testing Procedure  

Suppose $H := K (X^T X)^g K^T$, then 

$$
\begin{aligned}
(K \beta)^T (\sigma^2 H)^{-1} (K \hat{\beta}) &\sim \chi^2_{\text{df} = \text{rank}(H)}(\lambda) \\
\end{aligned}
$$

where the non-centrality parameter $\lambda = \frac{1}{2} (K \beta)^T (\sigma^2 H)^{-1}(K \beta)$ is the well-known distributional result of a normal quadratic form. 

Finally, our F Statistic: 

$$
\begin{aligned}
F := \frac{\left((K \beta)^T (\sigma^2 H)^{-1}(K \beta)\right) / \text{rank}(H)}{\text{RSS}/(n-p)}
\sim \frac{\chi^2 (\lambda)}{\chi^2} \sim F_{\text{rank}(H), n-p}(\lambda)
\end{aligned}
$$

We have successfully constructed a statistical test that allows us to test our hypothesis with a simple F-test. In `R`, we can use the `anova()` function to perform this test.

## Results 

```{r}
# Load the data
#anova.tbl <- read_csv("anova_results.csv")

# Print 
#kableExtra::kable(anova.tbl, 
#  caption = "F-Test Results for the Hypothesis Test")
```
![F-Test Results](anova-results.png)

The result shows that under the null hypothesis, the probability of getting a more extreme result than our calculate F-test statistics $\text{Pr}(>F)$ is $2.2e− 16$. 

This evidence would lead us to reject the null hypothesis and conclude that our subset of predictors is indeed a significant predictor of exam scores

# Elastic Net

# Why Use Elastic Net?

- **Limitations of Lasso**: May select only one variable from a group of highly correlated predictors.
- **Limitations of Ridge**: Cannot produce sparse models (i.e., no feature selection).
- **Elastic Net Advantage**: 
  - Encourages group selection.
  - Balances sparsity and multicollinearity handling.


# Elastic Net Formula

Elastic Net adds two penalty terms:

$$ \min_{\beta} \Bigg( \sum_{i=1}^n (y_i - X_i \beta)^2 + \lambda_1 \|\beta\|_1 + \lambda_2 \|\beta\|_2^2 \Bigg) $$

- $\|\beta\|_1$: Lasso penalty (L1).
- $\|\beta\|_2^2$: Ridge penalty (L2).
- $\lambda_1, \lambda_2$: Regularization parameters.

# Tuning Parameters in Elastic Net
1. **$\alpha$**: Controls the mix between Ridge and Lasso.
   - $\alpha = 0$: Ridge.
   - $\alpha = 1$: Lasso.
   - $0 < \alpha < 1$: Elastic Net.
2. **$\lambda$**: Controls the overall strength of regularization.

### Grid Search:
- Perform cross-validation to find optimal values of $\alpha$ and $\lambda$.


# Elastic Net: Geometric Interpretation

- Elastic Net creates a penalty region combining L1 (diamond) and L2 (circle).
- Encourages sparsity while handling correlated features.

![Elastic Net compared to Lasso and Ridge Regression](visualization_Elastic.png)


# Elastic Net with Continuous Outcome

The $R^2$ on the test data is calculated to be approximately $76\%$, meaning our model is able to explain $76\%$ of the variance in Exam Score in the test dataset

![The first 14 rows of the Elastic Net coefficients table](elastnettable1.png){width=40%}

**Note:** Elastic Net (or Lasso) did not drop any variables as all predictors contribute to reducing the loss function, even with regularization applied.

# Elastic Net with Binary Outcome

![](ROC_3plots.png){width=100%}
**Note:** This is an imbalanced dataset as most of the students have scored more than 60 in the exams. The median (and the mean) of the dataset is very close to the 3rd quantile (69). For instance, if we use threshold = 70, we can predict the probability a student's score is within/out the top 25% of the scores almost perfectly.

# Stochastic Gradient Boosting Machine (GBM) - Gradient Boosting Machine Algorithm

# What is Gradient Boosting?

- **Definition**: Gradient boosting is a machine learning ensemble technique (ensemble models combine predictions from multiple base models to enhance overall performance) that sequentially combines the predictions of multiple weak learners, typically decision trees. 


- **Purpose**: It aims to improve overall predictive performance by optimizing the model’s weights based on the errors of previous iterations, gradually reducing prediction errors and enhancing the model’s accuracy. 

# How It Works



- **Step 1**: Start with a baseline model (e.g., mean prediction for regression)
- **Step 2**: Compute residuals or errors from the current model
- **Step 3**: Fit a new model to the residuals (weak learners like decision trees)
- **Step 4**: Update the overall model by adding the new learner
- **Step 5**: Repeat until convergence or a predefined number of iterations

![ ](gbm_visual.png){width=75%}

# In our dataset

![Feature Importance Summary](gbm_visual3.png){width=80%}

- **Feature**: Lists the features (variables) in the dataset.
- **Gain**: Contribution of the feature to the model’s accuracy. Higher values indicate greater importance. **Attendance** contributes the most (0.4234).
- **Cover**: Proportion of samples impacted by the feature during splits. Higher values mean broader impact. **Attendance** has the highest Cover (0.1493).
- **Frequency**: How often the feature is used in tree splits. Higher values suggest frequent use. **Hours_Studied** is split most often (0.1484).

# In our dataset (cont.)

### Key Insights:
- **Top Features**: "Attendance" and "Hours_Studied" are the most impactful features.
- **Low-Impact Features**: Features like "Motivation_LevelLow" contribute minimally and may be less relevant.

![](gbm_visual2.png){width=80%}












